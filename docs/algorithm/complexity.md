---
title: 算法复杂度
summary: 算法复杂度
authors:
    - Zhiyuan Chen
date: 2019-08-06 21:26:39
categories: 
    - algorithm
tags:
    - algorithm
---

# 复杂度

算法复杂度对于算法来说自然是最重要的了。这其中又分 **时间复杂度** 和 **空间复杂度** 。时间复杂度表示需要多久去计算，空间复杂度则表示需要多少内存来完成计算。由于目前计算机技术的发展，空间复杂度的重要性越来越低，而时间复杂度的重要性越来越高，所以当我们提到复杂度的时候，一般都表示时间复杂度。我们通常通过 **渐进分析（Asymptotic Analysis）** 来分析算法的复杂度。

让我们从一个简单的\(n\)位数乘法运算例子来开始这一小节吧。

传统计算机是如何进行乘法运算的呢？按位相乘？不这有些太传统了。分治法（Divide and Conquer）？对，没有什么比分治法最简单得了，将一个复杂的大问题转换成多个简单的小问题，多么美妙！对于\(n\)位数\(x\)和\(y\)，我们可以将它们之间的乘法运算\(x \times y\)转换为\((a \times 10^\frac{n}{2} + b) \times (c \times 10^\frac{n}{2} + d)\) = \((ac) \cdot 10^n + (ad + bc) \cdot 10^\frac{n}{2} + bd\)。就这样，通过几步简单的操作，我们就将一个\(n\)位数相乘问题转换成了四个\(\frac{n}{2}\)位数相乘问题。只是，他的复杂度稍微高了一些，有\(O(n{^2})\)。那么，我们如何能优化他呢？通过观察，我们可以发现\(ad + bc = (a + b) \cdot (c + d) - ac - bd\)。由于\(ac\)、\(bd\)都是我们计算所需要的，我们可以通过替代来将这里的两个乘法缩减到一个新乘法，最终转换为：\((ac) \cdot 10^n + ((a + b) \cdot (c + d) - ac - bd) \cdot 10^\frac{n}{2} + bd\)。这个式子看起来要更复杂一些，但实际上我们将这个\(n\)位数相乘问题转换成了三个\(\frac{n}{2}\)位数相乘问题，使得时间复杂度降低到了\(O(n{^\log{_2}3})\)（如果你在这里还一头雾水的话，请不要担心，我们稍后会详细解释）。这个算法也被称作Karatsuba算法，与之类似的还有Toom-Cook算法，他将这个\(n\)位数相乘问题转换成了五个\(\frac{n}{3}\)位数相乘问题，使得时间复杂度进一步降低到了\(O(n{^\log{_3}5})\)。我们强烈建议你试试能不能自己去找到如何完成这样的分解。此外，如果你有志于算法研究成为一代研究科学家的话，Arnold Schönhage和Volker Strassen于1971年提出使用快速傅里叶变换（FFT）进行乘法运算的SSA（Schönhage-Strassen algorithm）将乘法运算提升到了多项式时间，今年早些时候，David Harvey和Joris van der Hoeven发布的论文[Integer multiplication in time O(n log n)](https://hal.archives-ouvertes.fr/hal-02070778/document)将乘法运算的速度提升到了目前的理论极限 -- \(O(n log n)\)。

## 符号

我们刚才提到了算法的复杂度是\(O(n log n)\)，这又是什么意思呢？

大O符号，又称渐进符号，是德国数学家1892年引入的。除此之外还有大Ω符号与大Θ符号（一个有趣的事实，大O符号其实应该是大Ο符号（希腊字母，Omicron，但是因为Ο和O没什么视觉区别，所以一般会直接说O。

### 时间频度

时间频度使用\(T(n)\)符号表示，他代表的是一个算法执行所需要花费的时间。有些人说时间频度是无法计算的……这个大家看看就好。他和\(O(n)\)的区别主要在于常数项 -- \(T(n^2 + 2n + 1)\) = \(O(n^2)\) = \(T(66n^2 + 66n + 66)\)。

### 大O符号

对于两个在任意正实数的无界子集上定义的关于\(n\)的实数或复数函数\(T(n)\)\(和实数函数f(n)\)，当且仅当存在正实数\(k\)和\(n{_0}\)，使得对于所有足够大的\(n\)，\(n>n{_0}\)，都有\(\left|T(n)\right| \leq k \times f(n)\)，那么我们可以说当\(n \to \infty\)时，\(T(n) = O(f(n))\)。

简而言之，大O符号表示一个算法的最坏情况。

### 大Ω符号

对于两个在任意正实数的无界子集上定义的关于\(n\)的实数或复数函数\(T(n)\)\(和实数函数f(n)\)，当且仅当存在正实数\(k\)和\(n{_0}\)，使得对于所有足够大的\(n\)，\(n>n{_0}\)，都有\(\left|T(n)\right| \geq k \times f(n)\)，那么我们可以说当\(n \to \infty\)时，\(T(n) = Ω(f(n))\)。

简而言之，大Ω符号表示一个算法的最好情况。

### 大Θ符号

对于两个在任意正实数的无界子集上定义的关于\(n\)的实数或复数函数\(T(n)\)\(和实数函数f(n)\)，当且仅当存在正实数\(k{_1}\)、\(k{_2}\)和\(n{_0}\)，使得对于所有足够大的\(n\)，\(n>n{_0}\)，都有\(k{_1} \times f(n) \leq \left|T(n)\right| \leq k{_2} \times f(n)\)，那么我们可以说当\(n \to \infty\)时，\(T(n) = Θ(f(n))\)。

显而易见，大Θ符号即是大O符号和大Ω符号的和，表示一个算法的区间。

考虑到很多算法的最好情况一致而最坏情况不一致（比如快速排序和归并排序都为\(Ω(n\log{_2}n)\)，但快速排序是\(O(n{^2})\)，而归并排序是\(O(n\log{_2}n)\)）。以及更多情况下，我们会遇到最坏情况。所以说到复杂度的时候我们都会说O，甚至很多时候我们会说最好情况是\(O(n\log{_2}n)\)，而非使用\(Ω(n\log{_2}n)\)。

## 时间

一般情况下，算法的速度不会比\(O(n)\)更快，因为读取数据即需要\(Θ(n)\)的时间，我们也把\(O(n)\)称作线性时间。此外，有很少人会把\(O(\log n)\)称作对数时间。对于大于一的实数c来说，我们一般会把\(O(n^c)\)称作多项式时间，把\(O(c^n)\)称作指数时间。在这更之上的是被称为阶乘时间的\(O(n!)\)。多项式时间是算法速度的分界线，即当一个算法可以在多项式时间内完成的时候，我们会说他是一个速度很快的算法。事实上，大多数算法也都能在多项式时间内完成。我们将能在多项式时间内解决的问题称作P（polynomial）问题，能在多项式时间内验证答案的问题称作NP（nondeterministic polynomial）问题。一个著名的NP完全问题 -- 旅行商问题，他的动态规划的解法为指数时间，而搜索的解法则是阶乘时间。有关NP难（NP-hard）问题和NP完全（NP Complete）问题，我们将在P=NP文章中阐述，有关动态规划、搜索的问题，我们将在相应的文章当中阐述。有极少的算法的复杂度仅为\(O(1)\)，比如判断一个数字的奇偶，我们称之为常数时间。但是我更喜欢把这种算法称为操作，因为他并没有完成什么计算。需要注意的是，算法要么很快（多项式时间），要么很慢（指数时间），并没有不快不慢的算法。

## 分析

说了这么多，终于到了这最重要的一步了。如果分析一个算法的复杂度呢？

用我们之前的Karatsuba算法举例来说：

    public static int karatsuba(x, y)
    {
        if(n==1) { return xy }
        a * 10^(n/2) + b = x
        c * 10^(n/2) + d = y
        ac = karatsuba(a, c)  // Repeat
        bd = karatsuba(b, d)  // Repeat
        temp = karatsuba(a + b, c + d) // Repeat
        return ac * 10^n + (temp - ac - bd) * 10^(n/2) + bd
    }

我们可以观察到，注释注明Repeat的三行代码是不断重复的，即每次分治将一个问题变成三个问题，同时每次分治问题的大小变为原来的一半。

最终，这个问题会转换为\(3^t\)个\(\frac{n}{2^t}\)个计算，那么复杂度即为\(O(3{^\log{_2}n})\)，也即\(O(n{^\log{_2}3})\)。

很简单，不是吗？
